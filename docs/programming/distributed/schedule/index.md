# 分布式调度系统设计关键点
## 调度层：分布式调度 + 分片 + 异步化，提升触发吞吐量
   **高峰期的首要瓶颈是「调度触发」—— 大量任务集中在凌晨 00:00-02:00 触发，单 Master 节点无法承载高并发调度请求。**

- Master 集群水平扩展：
去中心化支持部署多个 Master 节点，通过 ZK 实现任务分片（MasterShardingService），每个 Master 仅负责一部分调度任务（按工作流 ID 哈希分片），分散调度压力。
源码中 MasterSchedulerThread 会定期从数据库读取「待触发任务」，并根据分片规则过滤出本节点负责的任务，避免 Master 间重复处理。

- 调度异步化：
将「调度触发」和「任务提交」解耦：调度线程仅负责计算触发时间、校验依赖，然后将任务提交到异步队列（AsyncTaskSubmitQueue），由专门的提交线程池（TaskSubmitExecutor）将任务发送到 Worker 节点，避免调度线程阻塞。

- 预调度机制：
新增「预调度」（PreScheduleService），提前 5-10 分钟将凌晨高峰期的任务加载到内存调度队列，避免高峰期数据库查询压力，提升触发响应速度。

- 缓存引入：
DAG引入「依赖缓存」（DependencyCacheManager），将频繁查询的依赖状态缓存到本地内存 + Redis，解析耗时从 100ms 级降至 10ms 级，减少数据库压力

## 分发层：提升分发效率

- 分发逻辑优化：原任务队列的实现基于 ZooKeeper。master 将任务数据存放到 ZooKeeper 中，然后 worker 节点通过分布式锁的方式去消费任务队列，延迟了任务开始执行的时间。为保证任务队列的性能，ZooKeeper 的节点中并未存储执行任务所需的全部数据。许多任务的元数据如租户，队列和任务实例信息等都需要由 worker 操作数据库进行获取，增加了数据库的负担。
改进为任务队列基于 Netty 实现，master 保留了原有的逻辑，当 master 节点切分出任务节点后，使用配置的任务分发策略直接发送目标 worker 节点进行执行。worker 节点在启动的时候将节点信息和分组信息注册到 ZooKeeper 中，供 master 节点进行调用。性能优化的核心是去除了 worker节点的 ZooKeeper 操作和数据库操作。 结合Kafka，留存任务分发全量日志

- 任务分配召回策略：
一次启动100个任务时，大量任务可能会在短时间内全部分配给负载最轻（0.1）的Worker，导致其不堪重负，其他Worker却空闲
Master在选择Worker时，策略有所优化：
如果Worker的等待执行队列为空，则将任务分配给负载最小的Worker。
如果Worker的等待队列不为空，则将任务分配给等待队列最短的Worker；若等待队列长度相等，再依据负载选择。
如果所有Worker的等待队列均已满，则Master会阻塞1秒后再次尝试选择Worker。
队列状态同步与监控：
Worker会通过定期向ZooKeeper发送心跳，更新其等待执行队列的长度等信息。Master通过监听ZooKeeper来感知各Worker的实时负载。
召回触发与任务重分配：
当Worker端的执行队列和等待执行队列都满时，等待分配队列会每隔1秒尝试分配一次任务。如果等待分配队列也满了Worker1无法处理新任务，向Master通过心跳感知到其过载状态，就会触发Master召回机制。此时，Worker会把无法处理的任务返回给Master，Master则会重新尝试将这些任务分配给其他可能空闲的Worker


## 执行层：Worker 集群扩容 + 任务分片 + 并行执行，提升任务处理能力
   调度触发后，大量任务需要在 Worker 节点执行，高峰期的核心瓶颈是「Worker 资源不足」和「任务执行串行化」。

- Worker 集群弹性扩容：
支持动态添加 Worker 节点（无需重启集群），WorkerRegistryService 会自动将新节点注册到集群，Master 会通过负载均衡（LoadBalanceStrategy，支持轮询 / 加权轮询 / 最小负载）将任务分发到空闲 Worker。
4.x 版本支持「K8s 弹性伸缩」（K8sWorkerScaler），基于 Worker 节点的 CPU / 内存使用率、任务等待队列长度，自动扩容 / 缩容 K8s Pod 数量，高峰期秒级扩容，低谷期自动缩容节省资源。
5.引入「标签路由」（LabelRouter），支持将任务调度到指定标签的 Worker 节点（如核心任务调度到高性能节点），进一步强化资源隔离

- 任务分片与并行执行：
支持将大任务拆分为多个分片（ShardingTask），如按日期、地区、用户 ID 分片，分片任务在多个 Worker 节点并行执行（ShardingTaskExecutor），执行效率提升 N 倍（N 为分片数）。
源码中 ProcessInstance 会记录分片任务的执行状态（ShardingStatus），Master 实时监控所有分片进度，全部完成后才标记任务成功。

- 任务优先级调度：
高峰期任务按优先级排序（TaskPriority，支持手动设置 / 自动计算），核心任务（如核心报表任务）优先占用 Worker 资源，非核心任务延后执行，保障核心基线稳定。

- Worker 引入「任务执行线程池」（WorkerExecutorService），每个 Worker 可配置多个执行线程（默认 10 个，支持动态调整），单 Worker 节点可并行执行多个任务，并发能力提升 5-10 倍。

## 存储层：读写分离 + 缓存 + 分库分表，解决数据库瓶颈
   高峰期大量任务的触发、状态更新会导致数据库（MySQL/PostgreSQL）读写压力激增，出现连接超时、SQL 执行缓慢等问题，影响调度流程。

- 读写分离：
支持数据库读写分离（DataSourceRouting），调度触发、任务状态查询等读操作路由到从库，任务提交、状态更新等写操作路由到主库，分散数据库压力。
DynamicDataSource 根据 SQL 类型（SELECT/INSERT/UPDATE）自动切换数据源，确保读写分离生效。

- 多级缓存：
一级缓存：Master/Worker 本地内存缓存（LocalCache），缓存工作流定义、任务配置、依赖状态等高频访问数据，有效期 5-10 分钟，减少数据库查询。
二级缓存：Redis 分布式缓存（RedisCache），缓存跨节点共享数据（如集群节点状态、任务队列信息），支持缓存失效自动刷新。

- 分库分表：
按「时间」「工作流 ID」对核心表（如 t_ds_process_instance「t_ds_task_instance」）进行分库分表（ShardingSphere 集成），避免单表数据量过大（千万级以上）导致的 SQL 执行缓慢。
分库分表后，单表数据量控制在百万级以内，SQL 执行耗时降至毫秒级，数据库并发处理能力提升 10 倍

## 任务优化：轻量化执行 + 超时控制 + 死信队列，避免任务堆积
   凌晨高峰期部分任务可能存在「执行时间过长」「资源消耗过大」「死循环」等问题，导致 Worker 节点被占满，任务堆积。

- 轻量化任务执行：
支持「代理执行」模式（ProxyExecution），对于简单任务（如 Shell 脚本、SQL 执行），无需启动独立进程，直接在 Worker 线程中执行，减少进程创建销毁开销。
支持「任务池化」（TaskPool），复用任务执行进程，避免大量任务并发时的进程资源耗尽。

- 超时控制与中断：
支持为每个任务配置执行超时时间（TaskTimeout），超时后 TaskTimeoutMonitor 会自动中断任务（杀死进程 / 线程），释放 Worker 资源。
TaskExecutor 会定期检查任务执行时间，超过阈值则触发中断逻辑，并标记任务为「超时失败」，支持重试或人工处理。

- 死信队列：
对于多次重试仍失败的任务（如数据源持续异常、任务逻辑错误），会被放入死信队列（DeadLetterQueue），不再占用正常任务队列资源，避免影响其他任务执行。死信队列机制减少了无效重试，Worker 资源利用率提升 30% 以上，任务堆积概率降低 80%。
