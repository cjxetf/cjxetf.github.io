import{_ as o,I as t,c as i,o as s,j as r,J as l,a9 as n,a as p}from"./chunks/framework.Dp2GimK2.js";const _=JSON.parse('{"title":"分布式调度系统设计关键点","description":"","frontmatter":{},"headers":[],"relativePath":"programming/distributed/schedule/index.md","filePath":"programming/distributed/schedule/index.md","lastUpdated":1765356475000}'),d={name:"programming/distributed/schedule/index.md"};function k(c,e,h,u,W,m){const a=t("ArticleMetadata");return s(),i("div",null,[e[0]||(e[0]=r("h1",{id:"分布式调度系统设计关键点",tabindex:"-1"},[p("分布式调度系统设计关键点 "),r("a",{class:"header-anchor",href:"#分布式调度系统设计关键点","aria-label":"Permalink to “分布式调度系统设计关键点”"},"​")],-1)),l(a),e[1]||(e[1]=n('<h2 id="调度层-分布式调度-分片-异步化-提升触发吞吐量" tabindex="-1">调度层：分布式调度 + 分片 + 异步化，提升触发吞吐量 <a class="header-anchor" href="#调度层-分布式调度-分片-异步化-提升触发吞吐量" aria-label="Permalink to “调度层：分布式调度 + 分片 + 异步化，提升触发吞吐量”">​</a></h2><p><strong>高峰期的首要瓶颈是「调度触发」—— 大量任务集中在凌晨 00:00-02:00 触发，单 Master 节点无法承载高并发调度请求。</strong></p><ul><li><p>Master 集群水平扩展： 去中心化支持部署多个 Master 节点，通过 ZK 实现任务分片（MasterShardingService），每个 Master 仅负责一部分调度任务（按工作流 ID 哈希分片），分散调度压力。 源码中 MasterSchedulerThread 会定期从数据库读取「待触发任务」，并根据分片规则过滤出本节点负责的任务，避免 Master 间重复处理。</p></li><li><p>调度异步化： 将「调度触发」和「任务提交」解耦：调度线程仅负责计算触发时间、校验依赖，然后将任务提交到异步队列（AsyncTaskSubmitQueue），由专门的提交线程池（TaskSubmitExecutor）将任务发送到 Worker 节点，避免调度线程阻塞。</p></li><li><p>预调度机制： 新增「预调度」（PreScheduleService），提前 5-10 分钟将凌晨高峰期的任务加载到内存调度队列，避免高峰期数据库查询压力，提升触发响应速度。</p></li><li><p>缓存引入： DAG引入「依赖缓存」（DependencyCacheManager），将频繁查询的依赖状态缓存到本地内存 + Redis，解析耗时从 100ms 级降至 10ms 级，减少数据库压力</p></li></ul><h2 id="分发层-提升分发效率" tabindex="-1">分发层：提升分发效率 <a class="header-anchor" href="#分发层-提升分发效率" aria-label="Permalink to “分发层：提升分发效率”">​</a></h2><ul><li><p>分发逻辑优化：原任务队列的实现基于 ZooKeeper。master 将任务数据存放到 ZooKeeper 中，然后 worker 节点通过分布式锁的方式去消费任务队列，延迟了任务开始执行的时间。为保证任务队列的性能，ZooKeeper 的节点中并未存储执行任务所需的全部数据。许多任务的元数据如租户，队列和任务实例信息等都需要由 worker 操作数据库进行获取，增加了数据库的负担。 改进为任务队列基于 Netty 实现，master 保留了原有的逻辑，当 master 节点切分出任务节点后，使用配置的任务分发策略直接发送目标 worker 节点进行执行。worker 节点在启动的时候将节点信息和分组信息注册到 ZooKeeper 中，供 master 节点进行调用。性能优化的核心是去除了 worker节点的 ZooKeeper 操作和数据库操作。 结合Kafka，留存任务分发全量日志</p></li><li><p>任务分配召回策略： 一次启动100个任务时，大量任务可能会在短时间内全部分配给负载最轻（0.1）的Worker，导致其不堪重负，其他Worker却空闲 Master在选择Worker时，策略有所优化： 如果Worker的等待执行队列为空，则将任务分配给负载最小的Worker。 如果Worker的等待队列不为空，则将任务分配给等待队列最短的Worker；若等待队列长度相等，再依据负载选择。 如果所有Worker的等待队列均已满，则Master会阻塞1秒后再次尝试选择Worker。 队列状态同步与监控： Worker会通过定期向ZooKeeper发送心跳，更新其等待执行队列的长度等信息。Master通过监听ZooKeeper来感知各Worker的实时负载。 召回触发与任务重分配： 当Worker端的执行队列和等待执行队列都满时，等待分配队列会每隔1秒尝试分配一次任务。如果等待分配队列也满了Worker1无法处理新任务，向Master通过心跳感知到其过载状态，就会触发Master召回机制。此时，Worker会把无法处理的任务返回给Master，Master则会重新尝试将这些任务分配给其他可能空闲的Worker</p></li></ul><h2 id="执行层-worker-集群扩容-任务分片-并行执行-提升任务处理能力" tabindex="-1">执行层：Worker 集群扩容 + 任务分片 + 并行执行，提升任务处理能力 <a class="header-anchor" href="#执行层-worker-集群扩容-任务分片-并行执行-提升任务处理能力" aria-label="Permalink to “执行层：Worker 集群扩容 + 任务分片 + 并行执行，提升任务处理能力”">​</a></h2><p>调度触发后，大量任务需要在 Worker 节点执行，高峰期的核心瓶颈是「Worker 资源不足」和「任务执行串行化」。</p><ul><li><p>Worker 集群弹性扩容： 支持动态添加 Worker 节点（无需重启集群），WorkerRegistryService 会自动将新节点注册到集群，Master 会通过负载均衡（LoadBalanceStrategy，支持轮询 / 加权轮询 / 最小负载）将任务分发到空闲 Worker。 4.x 版本支持「K8s 弹性伸缩」（K8sWorkerScaler），基于 Worker 节点的 CPU / 内存使用率、任务等待队列长度，自动扩容 / 缩容 K8s Pod 数量，高峰期秒级扩容，低谷期自动缩容节省资源。 5.引入「标签路由」（LabelRouter），支持将任务调度到指定标签的 Worker 节点（如核心任务调度到高性能节点），进一步强化资源隔离</p></li><li><p>任务分片与并行执行： 支持将大任务拆分为多个分片（ShardingTask），如按日期、地区、用户 ID 分片，分片任务在多个 Worker 节点并行执行（ShardingTaskExecutor），执行效率提升 N 倍（N 为分片数）。 源码中 ProcessInstance 会记录分片任务的执行状态（ShardingStatus），Master 实时监控所有分片进度，全部完成后才标记任务成功。</p></li><li><p>任务优先级调度： 高峰期任务按优先级排序（TaskPriority，支持手动设置 / 自动计算），核心任务（如核心报表任务）优先占用 Worker 资源，非核心任务延后执行，保障核心基线稳定。</p></li><li><p>Worker 引入「任务执行线程池」（WorkerExecutorService），每个 Worker 可配置多个执行线程（默认 10 个，支持动态调整），单 Worker 节点可并行执行多个任务，并发能力提升 5-10 倍。</p></li></ul><h2 id="存储层-读写分离-缓存-分库分表-解决数据库瓶颈" tabindex="-1">存储层：读写分离 + 缓存 + 分库分表，解决数据库瓶颈 <a class="header-anchor" href="#存储层-读写分离-缓存-分库分表-解决数据库瓶颈" aria-label="Permalink to “存储层：读写分离 + 缓存 + 分库分表，解决数据库瓶颈”">​</a></h2><p>高峰期大量任务的触发、状态更新会导致数据库（MySQL/PostgreSQL）读写压力激增，出现连接超时、SQL 执行缓慢等问题，影响调度流程。</p><ul><li><p>读写分离： 支持数据库读写分离（DataSourceRouting），调度触发、任务状态查询等读操作路由到从库，任务提交、状态更新等写操作路由到主库，分散数据库压力。 DynamicDataSource 根据 SQL 类型（SELECT/INSERT/UPDATE）自动切换数据源，确保读写分离生效。</p></li><li><p>多级缓存： 一级缓存：Master/Worker 本地内存缓存（LocalCache），缓存工作流定义、任务配置、依赖状态等高频访问数据，有效期 5-10 分钟，减少数据库查询。 二级缓存：Redis 分布式缓存（RedisCache），缓存跨节点共享数据（如集群节点状态、任务队列信息），支持缓存失效自动刷新。</p></li><li><p>分库分表： 按「时间」「工作流 ID」对核心表（如 t_ds_process_instance「t_ds_task_instance」）进行分库分表（ShardingSphere 集成），避免单表数据量过大（千万级以上）导致的 SQL 执行缓慢。 分库分表后，单表数据量控制在百万级以内，SQL 执行耗时降至毫秒级，数据库并发处理能力提升 10 倍</p></li></ul><h2 id="任务优化-轻量化执行-超时控制-死信队列-避免任务堆积" tabindex="-1">任务优化：轻量化执行 + 超时控制 + 死信队列，避免任务堆积 <a class="header-anchor" href="#任务优化-轻量化执行-超时控制-死信队列-避免任务堆积" aria-label="Permalink to “任务优化：轻量化执行 + 超时控制 + 死信队列，避免任务堆积”">​</a></h2><p>凌晨高峰期部分任务可能存在「执行时间过长」「资源消耗过大」「死循环」等问题，导致 Worker 节点被占满，任务堆积。</p><ul><li><p>轻量化任务执行： 支持「代理执行」模式（ProxyExecution），对于简单任务（如 Shell 脚本、SQL 执行），无需启动独立进程，直接在 Worker 线程中执行，减少进程创建销毁开销。 支持「任务池化」（TaskPool），复用任务执行进程，避免大量任务并发时的进程资源耗尽。</p></li><li><p>超时控制与中断： 支持为每个任务配置执行超时时间（TaskTimeout），超时后 TaskTimeoutMonitor 会自动中断任务（杀死进程 / 线程），释放 Worker 资源。 TaskExecutor 会定期检查任务执行时间，超过阈值则触发中断逻辑，并标记任务为「超时失败」，支持重试或人工处理。</p></li><li><p>死信队列： 对于多次重试仍失败的任务（如数据源持续异常、任务逻辑错误），会被放入死信队列（DeadLetterQueue），不再占用正常任务队列资源，避免影响其他任务执行。死信队列机制减少了无效重试，Worker 资源利用率提升 30% 以上，任务堆积概率降低 80%。</p></li></ul>',14))])}const M=o(d,[["render",k]]);export{_ as __pageData,M as default};
