

# RAG 系统优化指南

构建高质量 RAG 系统需从 **数据 → 分块 → 存储 → 检索 → 生成 → 评估** 全链路协同优化。以下是关键优化方向总结：

---

## 1. 知识库数据质量把控

- **去噪与清洗**：
    - 剔除明显噪声：广告、乱码、无关重复内容、模板化文本。
    - 可结合 **人工规则 + LLM 判断**（如让 LLM 识别“是否包含有效信息”）。
- **一致性处理**：
    - 统一术语（如“SpringAI” vs “Spring AI”）、格式（日期、单位）、编码。
- **敏感信息脱敏**：
    - 自动识别并移除 PII（个人身份信息）、密钥等。

> ✅ 高质量输入 = 高质量输出。垃圾进，垃圾出（GIGO）在 RAG 中尤为明显。

---

## 2. 分块策略（Chunking Strategy）

分块是 RAG 的**第一道瓶颈**，直接影响检索召回率与生成质量。

| 策略 | 说明 | 适用场景 |
|------|------|--------|
| **固定大小 + 重叠** | 按字符/Token 切分，相邻块保留 overlap（如 50 tokens）<br>常用分隔符：`["\n\n", "\n", "。", "？", "！", "；"]` | 通用文档，无明确结构 |
| **递归分块（Recursive）** | 优先按语义分隔符（段落 > 句子）切分；若仍超限，则强制截断 | 平衡语义完整性与长度控制（LangChain 默认） |
| **语义分块（Semantic）** | 利用 Embedding 计算相邻句子余弦相似度，低于阈值则切分<br>✅ **LangChain4j 已内置 `SemanticChunker`** | 话题转换频繁的长文本（论文、报告） |
| **文档结构感知分块** | 对 Markdown/HTML 等结构化文档：<br>- **切分层级**：按 H1/H2/H3 切<br>- **索引层级**：可选 H2 或 text（需 ≤ 切分层级）<br>⚠️ 过细 → 语义割裂 + 成本高；过粗 → 超 token 限制 + 检索不准 | 技术文档、API 手册、白皮书 |
| **LLM 辅助分块** | 提示 LLM 将长文本划分为“语义独立、自包含”的块 | 高价值文档，可接受较高成本 |

> 💡 **推荐组合**：结构化文档 → **H2 切分 + H2 索引 + 标题-摘要增强**

---

## 3. 向量存储设计

向量数据库不仅要存向量，更要存**丰富的标量元数据**以支持过滤与上下文重建。

### 必存字段
- `content`：原始文本片段
- `doc_id` / `file_path`：来源标识
- `chunk_index`：块序号

### 优化手段
- **摘要索引增强（Metadata Augmentation）**  
  利用 LLM 为文档/章节生成摘要，拼接到 chunk 内容中再向量化，提升检索相关性。

- **父子分段（Parent-Child Chunking）**
    - 子块（小粒度）用于向量检索
    - 父块（大粒度，如整节）用于最终生成
    - 存储 `parent_id`，检索后去重父 ID 再拉取完整上下文

- **标量过滤字段**   Dify 做不到
    - 权限控制：`allowed_roles: ["admin", "user"]`
    - 多租户隔离：`tenant_id: "company_a"`
    - 业务标签：`category: "installation"`

> ✅ 向量 + 标量 = 精准检索 + 安全可控

---

## 4. 查询优化（Query Optimization）

用户提问往往模糊、简短或含歧义，需主动优化：

- **查询扩展（Query Expansion）**
    - 同义词扩展、关键词提取、问题拆解（如“如何安装并配置？” → 拆为“安装步骤”+“配置方法”）

- **假设性文档嵌入（HyDE, Hypothetical Document Embeddings）**
    1. 让 LLM 生成一个**假设性答案**
    2. 用该答案生成 embedding 进行检索
    3. 实际效果常优于原始 query（因更接近文档表述）

- **混合检索（Hybrid Search）**
    - **向量检索**（语义匹配） + **关键词检索**（BM25/全文搜索）
    - 加权融合结果（如 70% 向量 + 30% 关键词）
    - 若文档含丰富实体关系（如组织架构、权限模型），可引入 **Neo4j 等知识图谱**，实现 **GraphRAG**（微软开源方案）

---

## 5. Embedding 与 ReRank 模型选型

- **Embedding 模型**：
    - 中文首选：**BGE 系列**（`bge-large-zh-v1.5`）、**Qwen-Embedding**（通义千问）
    - 英文/多语言：**text-embedding-3-large**（OpenAI）、**voyage-lite**
    - 参考权威榜单：[MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

- **ReRank 模型（可选但强烈推荐）**：
    - 对 Top-K 检索结果用 Cross-Encoder 重排（如 `bge-reranker`, `Cohere Rerank`）
    - 显著提升排序精度，尤其对复杂查询

---

## 6. 向量数据库选型对比

| 特性 | **Milvus / Zilliz** | **Elasticsearch (ES)** |
|------|-------------------|----------------------|
| 定位 | 专为向量检索设计 | 全文检索 + 向量 + 结构化数据一体化 |
| ANN 算法 | HNSW, IVF-PQ（高度优化） | HNSW（8.0+ 支持） |
| 性能 | ⭐⭐⭐⭐⭐（亿级向量毫秒响应） | ⭐⭐⭐（中小规模足够） |
| 标量过滤 | 支持，但早期版本较弱 | ⭐⭐⭐⭐⭐（DSL 强大，天然支持复杂过滤） |
| 混合搜索 | 需额外集成 ES | 原生支持 BM25 + 向量融合 |
| 运维复杂度 | 较高（需独立集群） | 较低（若已有 ES） |

> ✅ **建议**：
> - 纯向量场景 → **Milvus**
> - 需要关键词 + 向量 + 权限过滤 → **Elasticsearch**

---

## 7. 查询后置处理（Post-Retrieval Processing）

检索结果 ≠ 直接喂给 LLM，需精炼：

- **去重与过滤**：移除重复 chunk，按权限/租户过滤
- **上下文截断**：按 LLM token 限制，优先保留高相关性块
- **重排（ReRank）**：使用专用 reranker 模型调整顺序
- **Prompt 构建**：
  ```text
  【用户问题】{query}
  【对话历史】{history}
  【参考文档】
  - {doc1}
  - {doc2}
  请基于以上信息回答，若信息不足，请明确说明“未找到相关信息”。
- 置信度控制：
  若 top chunks 相似度低 → 返回“信息不足”而非幻觉
  可附带引用来源（“根据《安装指南》第3节...”）

## 8. Agentic RAG（智能体驱动 RAG）
   这是 RAG 的高级演进形态：将整个流程交由 LLM Agent 动态决策。

工作流程
Agent 接收用户问题
自主判断：是否需多轮检索？是否需拆解子问题？
动态执行：调用工具（检索、计算、API）
整合信息：过滤、去重、推理
生成答案：带引用、可解释
> 🌟 优势：模拟人类研究过程，极大提升复杂、多跳、规划类问题的准确率。

## 9. RAG 核心评估指标优化指南

| 评估指标 | 典型问题表现 | 主要反映环节 | 首要优化方向 | 具体优化措施 |
|----------|--------------|--------------|----------------|----------------|
| **Hit Rate@K**<br>（Top-K 命中率） | “完全没找到相关文档”<br>用户问 A，返回 B | 检索召回能力 | ✅ 分块策略 + Embedding 模型 | • 使用结构感知分块（如 H2 切分）<br>• 采用语义分块（Semantic Chunking）<br>• 应用“标题-摘要-内容”拼接增强<br>• 升级 Embedding 模型（中文：`bge-large-zh` / `Qwen-Embedding`）<br>• 添加查询改写（LLM 扩展 query）<br>• 引入混合检索（向量 + BM25） |
| **MRR@K**<br>（平均倒数排名） | “找到了，但排在第 5 名以后”<br>相关 chunk 被埋没 | 检索排序质量 | ✅ ReRank 模型 + 查询优化 | • 引入 Cross-Encoder 重排（如 `bge-reranker`）<br>• 使用 HyDE（假设性文档嵌入）生成更匹配的 query embedding<br>• 在 chunk 中注入父级标题/摘要，提升语义聚焦度<br>• 增大初始召回数（如 Top-50 → ReRank → Top-5） |
| **Recall@K**<br>（多答案召回率） | “只答出一部分”<br>问题有多个正确答案但漏掉 | 多答案覆盖能力 | ✅ 多路召回 + 分块粒度 | • 扩大 K 值（检索更多候选）<br>• 多路召回融合：向量 + 关键词 + 知识图谱（如 Neo4j）<br>• 对含多知识点的 chunk 拆分为子块<br>• 使用父子分段：子块用于召回，父块用于覆盖完整信息 |
| **Faithfulness**<br>（忠实度） | “答案里有文档没提的内容”<br>出现幻觉、编造数据 | 生成可靠性 | ✅ Prompt 约束 + 上下文质量 | • 强化 prompt 指令：<br>  “仅基于以下文档回答，若无信息请说明”<br>• 确保检索 chunk 语义完整（避免半截句子）<br>• 后处理校验：用 LLM 判断 claim 是否被 context 支持<br>• 选用指令遵循能力强的 LLM（如 GPT-4、Claude） |
| **Answer Relevancy**<br>（答案相关性） | “答非所问”或“太啰嗦”<br>未直接解决用户问题 | 生成实用性 | ✅ Prompt 设计 + 检索相关性 | • 明确要求：“直接回答，不超过 3 步，不要背景介绍”<br>• 确保检索内容高度匹配问题意图<br>• 在 prompt 中整合对话历史<br>• 对 LLM 原始输出做后处理精炼（摘要/结构化） |

> 💡 **优化优先级建议**：  
> **1. 先确保 Hit Rate@3 ≥ 70%**（否则其他优化收益有限）→ 聚焦分块 + embedding  
> **2. 再提升 MRR/Recall** → 引入 ReRank + 查询优化  
> **3. 最后打磨 Faithfulness & Relevancy** → 优化 prompt + 后处理

## 10. RAG or 微调
| 场景维度         | 推荐方案       | 核心理由                                     |
|------------------|----------------|----------------------------------------------|
| 动态数据/实时知识 | RAG            | 知识库可随时增删改查，成本极低，无需重新训练模型。 |
| 对抗模型幻觉     | RAG > 微调     | 答案基于外部提供的确凿证据，有据可查，可溯源。   |
| 可解释性         | RAG            | 可以清晰展示用于生成答案的原始文本片段。       |
| 成本             | RAG            | 避免了昂贵的模型训练和部署成本，迭代速度快。   |
| 依赖通用能力     | RAG            | 充分利用基础模型的强大通用能力，仅外挂知识。   |
| 延迟             | 微调           | RAG增加了检索步骤，会引入额外的毫秒级延迟。     |
| 模型能力定制     | 微调           | 旨在改变模型的内在行为、风格或掌握新技能。     |
