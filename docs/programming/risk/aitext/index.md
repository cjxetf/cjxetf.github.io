# AI 社交内容风控


​**系统化设计一套面向流式大模型（如 LLM 聊天机器人）的内容风控方案**​，覆盖 **输入过滤 → 生成约束 → 流式监控 → 动态干预 → 输出兜底** 全链路，并重点强化 **动态 Prompt 调整、滑动窗口检测、优雅中断** 等关键能力。

---

## 🎯 一、整体架构：四阶段流式风控体系

> ✅ ​**核心目标**​：
> 
> * ​**高危内容零输出**​（如自杀方法、暴力细节）
> * ​**风险内容被“安全重写”**​（如引导求助、抽象化描述）
> * ​**用户体验不突兀**​（优雅中断 + 完整语句）

---

## ✅ 二、输入风控（Pre-Generation）

### 🔹 1. 多层输入过滤



| 层级                    | 技术                    | 作用                                                         |
| ------------------------- | ------------------------- | -------------------------------------------------------------- |
| **L1 字面量**     | AC 自动机               | 拦截 `"跳楼"`、`"割腕"`、`"氰化物"` 等明确高危词 |
| **L2 正则变体**   | Hyperscan (DFA+NFA)     | 匹配 `"怎么si"`、`"想不开的方法"`、`"自sha方式"` |
| **L3 分类小模型** | BERT-base 微调          | 判断意图：`[正常, 自残, 色情, 暴力, 政治]`               |
| **L4 上下文感知** | 用户历史行为 + 对话状态 | 若用户连续 3 条含 `"活着好累"` → 提升风险等级           |

### 🔹 2. 输入分档与处置



| 风险等级                 | 判定条件             | 处置策略                                                   |
| -------------------------- | ---------------------- | ------------------------------------------------------------ |
| **高危（拒绝）**   | 含具体自杀/暴力方法  | 直接返回：`{"code": 403, "msg": "该内容违反安全策略"}` |
| **高风险（引导）** | 表达自残意图但无方法 | 注入 **强约束 Prompt** + 启动生成                    |
| **中低风险**       | 色情/暴力模糊表达    | 注入 **限制性 Prompt**                               |
| **安全**           | 无风险               | 正常生成                                                   |

---

## ✅ 三、生成前：动态 Prompt 强约束（核心！）

> 💡 **关键：Prompt 不是固定的，而是根据输入风险动态组装**

### 🔧 动态 Prompt 模板示例

#### 🌪 场景 1：自残风险（高风险）



```
你是一个专业的心理援助 AI，请严格遵守以下规则：
1. **禁止描述任何自残、自杀的具体方法、工具或步骤**
2. **必须表达共情**："我能感受到你现在很痛苦..."
3. **必须提供求助建议**："建议联系心理热线：400-161-9995"
4. **禁止评判或说教**，只提供支持

用户说："我觉得活着没意思"
你的回复应聚焦于：倾听 + 共情 + 引导向专业帮助
```

#### 🌪 场景 2：色情风险（中风险）



```
你是一个健康向上的 AI 助手，请注意：
- 若涉及两性话题，**仅允许抽象、科普式描述**
- **禁止任何露骨、暗示性、挑逗性语言**
- 如无法安全回答，请回复："这个话题我不太适合讨论，我们可以聊聊别的吗？"
```

#### 🌪 场景 3：暴力风险



```
你坚决反对任何形式的暴力，请做到：
- **不描述暴力细节、工具、过程**
- **强调和平解决冲突**
- **引导至法律或调解途径**
```

> ✅ ​**实现方式**​：
> 
> * 预定义多套 Prompt 模板（YAML 配置）
> * 根据输入风控结果 **动态拼接 system prompt**
> * 通过 **LoRA 或 Prompt Tuning** 微调模型行为（可选）

---

## ✅ 四、流式输出风控（During Generation）

这是 AI 社交风控的​**最大挑战**​：LLM 是流式输出（token-by-token），需实时监控并干预。

### 🔹 1. Token 滑动窗口检测

* ​**窗口大小**​：最近 32～64 个 tokens（约 1～2 句话）
* ​**检测内容**​：
  * 新出现的敏感词（AC + Hyperscan 实时扫描）
  * 风险语义（轻量 NER + 关键词密度）
  * 违背 Prompt 约束（如输出了 `"你可以用刀..."`）

### 🔹 2. 风险分持续累计

为每个 token 计算风险分，​**指数衰减累计**​：



```
risk_score = 0.0
decay = 0.95  # 越早的 token 权重越低

for token in sliding_window:
    score = get_token_risk(token)  # 0.0 ～ 1.0
    risk_score = risk_score * decay + score

if risk_score > THRESHOLD_HIGH:   # 如 0.8
    trigger_intervention()
elif risk_score > THRESHOLD_MED:  # 如 0.5
    inject_safe_suffix()  # 流中追加安全结尾
```

### 🔹 3. 流中干预策略



| 风险等级       | 干预动作                                             | 用户体验                      |
| ---------------- | ------------------------------------------------------ | ------------------------------- |
| **高危** | **立即中断生成** + 返回预设关怀话术            | “我注意到你可能需要帮助...” |
| **中危** | **注入安全后缀**
`"...建议寻求专业支持。"` | 保持语句完整                  |
| **低危** | ​**记录日志**​，继续生成                     | 无感                          |

> ⚠️ ​**优雅中断技巧**​：
> 
> * 不在句子中间 cut，而是在标点后中断
> * 使用 **完整语义单元** 作为中断点（通过依存句法分析）

---

## ✅ 五、输出兜底（Post-Generation）

即使流中未触发，也要做最终校验：

### 🔹 1. 完整输出扫描

* 对最终完整回复，用 **AC + Hyperscan + 小模型** 全量扫描
* 若发现漏网之鱼 → **替换为安全模板**

### 🔹 2. 强制安全重写（Fallback）



```
if final_output_contains_risk():
    if suicide_intent_detected:
        return "我能感受到你的痛苦。请相信，有人愿意帮助你。心理援助热线：400-161-9995"
    elif violence_detected:
        return "暴力不能解决问题。建议通过沟通或法律途径化解矛盾。"
```

---

## ✅ 六、动态调整机制（重点！）

### 🔁 1. Prompt 动态优化闭环

### 🔁 2. 风险阈值自适应

* 根据 **误杀率 / 漏杀率** 动态调整 `THRESHOLD_HIGH`
* 高峰期适当放宽，深夜敏感时段收紧

---

## ✅ 七、技术栈推荐



| 模块                  | 推荐方案                                                  |
| ----------------------- | ----------------------------------------------------------- |
| **AC 自动机**   | Go: `cloudflare/ahocorasick`；Java: `darts-clone` |
| **多模正则**    | Intel Hyperscan（C++） + Python binding                   |
| **分类小模型**  | ONNX Runtime（BERT-base，< 50ms）                         |
| **流式监控**    | 自研 Token Stream Interceptor（Go/Python）                |
| **大模型推理**  | vLLM / Triton + TensorRT-LLM（支持流式）                  |
| **动态 Prompt** | 配置中心（Apollo/Nacos） + 热加载                         |

---

## ✅ 八、性能与延迟控制



| 阶段             | 延迟目标          | 优化手段                 |
| ------------------ | ------------------- | -------------------------- |
| 输入风控         | < 10ms            | AC + Hyperscan SIMD 加速 |
| 流式监控         | < 1ms/token       | 滑动窗口 + 增量匹配      |
| 安全重写         | < 5ms             | 预渲染话术模板           |
| **端到端** | **< 200ms** | 异步日志 + 批处理反馈    |

> 💡 ​**关键**​：流式监控必须 ​**轻量、无阻塞**​，避免拖慢生成速度。

---

## ✅ 总结：AI 社交风控黄金法则

> 🔐 **“输入严筛、Prompt 强约、流中盯防、输出兜底、动态进化”**

* ​**高危内容绝不输出**​（靠输入拦截 + 流中熔断）
* ​**风险内容必须重写**​（靠动态 Prompt + 安全模板）
* ​**一切可配置、可迭代**​（靠数据闭环 + AB 测试）
