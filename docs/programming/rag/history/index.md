# 多轮对话中 LLM 上下文超限的优化方案

在多轮对话系统中，随着交互轮数增加，历史消息不断累积，最终超过 LLM 的 token 上下文窗口限制（如 8K、32K 等），导致无法继续对话或关键信息被截断。

> **核心目标**：在有限上下文内，高效保留最有价值的对话历史，维持语义连贯性与任务完整性。

![img.png](img.png)

## ✅ 推荐方案（按优先级排序）

### 1. 对话摘要压缩【首选】

#### 原理
当历史接近上限时，用 LLM 自身对前面的对话进行摘要，用一段简短文字替代原始多轮记录。

#### 实现步骤
1. 监控当前上下文 token 数（使用 `tiktoken` 或模型 tokenizer）
2. 当达到阈值（如 70% 上限），触发摘要：
   ```text
   请将以下对话总结为一段简洁的背景描述，保留关键事实、用户意图和已达成的结论：

   [完整历史对话]
   ```
3. 将返回的摘要作为新的“系统记忆”，后续对话以此为基础追加新消息

#### 示例
- 原始历史（5 轮）：3000 tokens
- 摘要后：200 tokens
- 新上下文 = [摘要] + [最新1–2轮]

> 💡 进阶：采用分层摘要（每 N 轮摘要一次，形成摘要链）

### 2. 滑动窗口 + 关键轮次保留

   只保留最近 N 轮对话，但强制保留关键锚点轮次。

#### 实现
- 基础滑动：仅保留最后 3–5 轮（通常足够维持上下文）
- 锚点保留：
  - 用户首次提问（定义任务目标）
  - Tool 调用的输入/输出
  - 用户明确纠正或确认的信息（如 “不是 A，是 B”）
#### 示例结构
```python
history = [
{"role": "anchor", "content": "用户想订北京到上海的机票"},  # 锚点
{"role": "user", "content": "昨天说的航班有变动吗？"},
{"role": "assistant", "content": "CA1832 仍正常..."}
]
```
#### 适用场景
- 资源受限
- 通用聊天机器人

### 向量化记忆 + 动态检索（RAG for Chat History）

   将每轮对话嵌入向量库，每次生成前根据当前 query 检索最相关的过往片段。

#### 流程
- 每轮结束后，存入向量数据库
- 下次生成前：
  - 用当前用户消息做 embedding
  - 检索 Top-K 相关历史轮次
  - 拼接为：[检索结果] + [最近1–2轮]
#### 优势
  - 精准回忆早期细节
  - 支持话题跳跃


### 结构化状态管理（State Tracking）

   维护一个结构化对话状态对象，不传递原始对话。

#### 示例（订票场景）
```json

{
"task": "book_flight",
"slots": {
"origin": "Beijing",
"destination": "Shanghai",
"date": "2025-05-01",
"confirmed": false
},
"summary": "用户正在查询北京到上海5月1日的航班"
}
```
## 🛠️ 工程实践建议

1. **动态 Token 计算**
   使用模型对应的 tokenizer（如 Qwen 用 `tiktoken` + custom rules）实时统计 token。
2. **混合策略组合**
  * 日常对话：滑动窗口 + 锚点
  * 复杂任务：摘要压缩 + 结构化状态
  * 专业咨询：向量化记忆
3. **前端配合**
   UI 层也做历史折叠（如 “… earlier conversation summarized”），提升体验一致性。
4. **监控告警**
   记录因上下文截断导致的用户追问（如 “我之前说过…”），用于策略迭代。


## 开源方案 AutoContextMemory：智能上下文管理


面对这些挑战，AgentScope推出了AutoContextMemory ，它是 AgentScope Java 框架提供的智能上下文内存管理组件，通过自动压缩、卸载和摘要对话历史，在成本控制和信息保留之间找到最佳平衡。

![img_1.png](img_1.png)
### 核心价值

1. 自动压缩与智能摘要

当消息或 token 数量超过阈值时，自动触发 6 种渐进式压缩策略；
使用 LLM 智能摘要，保留关键信息而非简单截断；
无需人工干预，系统自动管理。
2. 内容卸载与完整追溯

将大型内容卸载到外部存储，通过 UUID 按需重载；
所有原始内容保存在原始存储中，支持完整历史追溯；
不会因为压缩而丢失任何信息。
3. 实际效果

✅ 成本降低 70%：通过智能压缩，大幅减少 token 使用量；
✅ 响应速度提升：更小的上下文意味着更快的处理速度；
✅ 信息保留完整：关键信息不会丢失，Agent 决策质量不受影响；
✅ 自动化管理：无需人工干预，系统自动优化。


### AutoContextMemory架构与工作原理

![img_2.png](img_2.png)
多存储架构

AutoContextMemory 采用多存储架构，确保在压缩的同时保留完整信息：

工作内存存储：存储压缩后的消息，直接参与模型推理，这是 Agent 实际使用的上下文；
原始内存存储：存储完整的、未压缩的消息历史，采用仅追加模式，支持完整历史追溯；
卸载上下文存储：以 UUID 为键存储卸载的消息内容，支持按需重载；
压缩事件存储：记录所有压缩操作的详细信息，用于分析和优化；
所有存储都支持状态持久化，可以结合 SessionManager 实现跨会话的上下文持久化。

6 种渐进式开箱即用压缩策略

AutoContextMemory 的核心是 6 种渐进式压缩策略。

压缩触发条件：

消息数量阈值 或者 Token 数量阈值,两个条件满足任一即触发压缩。

压缩流程：

检查阈值 → 策略1(压缩历史工具调用) → 策略2(卸载大型消息-带保护)
→ 策略3(卸载大型消息-无保护) → 策略4(摘要历史对话轮次)
→ 策略5(摘要当前轮次大型消息) → 策略6(压缩当前轮次消息)
压缩原则：

当前轮次优先：优先保护当前轮次的完整信息；
用户交互优先：用户输入和 Agent 回复的重要性高于工具调用的中间结果；
可回溯性：所有压缩的原文都可以通过 UUID 回溯，确保信息不丢失；
6 种压缩策略：

1. 压缩历史工具调用：查找历史对话中的连续工具调用消息（超过 6 条），使用 LLM 智能压缩，保留工具名称、参数和关键结果。轻量级策略，压缩成本低。
2. 卸载大型消息（带保护）：查找超过阈值的大型消息，保护最新的助手响应和最后 N 条消息，卸载原始内容并替换为预览和 UUID 提示。快速减少 token 使用。
3. 卸载大型消息（无保护）：与策略 2 类似，但不保护最后 N 条消息，仅保护最新的助手响应。更激进的压缩策略。
4. 摘要历史对话轮次：对历史用户-助手对话对进行智能摘要，使用 LLM 生成摘要保留关键决策和信息。大幅减少 token 使用。
5. 摘要当前轮次大型消息：查找当前轮次中超过阈值的大型消息，使用 LLM 生成摘要并卸载原始内容。针对当前轮次的优化。
6. 压缩当前轮次消息：压缩当前轮次的所有消息，合并多个工具结果，保留关键信息。最后的保障策略，确保上下文不超限。