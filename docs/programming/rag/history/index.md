# 多轮对话中 LLM 上下文超限的优化方案

在多轮对话系统中，随着交互轮数增加，历史消息不断累积，最终超过 LLM 的 token 上下文窗口限制（如 8K、32K 等），导致无法继续对话或关键信息被截断。

> **核心目标**：在有限上下文内，高效保留最有价值的对话历史，维持语义连贯性与任务完整性。

![img.png](img.png)

## ✅ 推荐方案（按优先级排序）

### 1. 对话摘要压缩【首选】

#### 原理
当历史接近上限时，用 LLM 自身对前面的对话进行摘要，用一段简短文字替代原始多轮记录。

#### 实现步骤
1. 监控当前上下文 token 数（使用 `tiktoken` 或模型 tokenizer）
2. 当达到阈值（如 70% 上限），触发摘要：
   ```text
   请将以下对话总结为一段简洁的背景描述，保留关键事实、用户意图和已达成的结论：

   [完整历史对话]
   ```
3. 将返回的摘要作为新的“系统记忆”，后续对话以此为基础追加新消息

#### 示例
- 原始历史（5 轮）：3000 tokens
- 摘要后：200 tokens
- 新上下文 = [摘要] + [最新1–2轮]

> 💡 进阶：采用分层摘要（每 N 轮摘要一次，形成摘要链）

### 2. 滑动窗口 + 关键轮次保留

   只保留最近 N 轮对话，但强制保留关键锚点轮次。

#### 实现
- 基础滑动：仅保留最后 3–5 轮（通常足够维持上下文）
- 锚点保留：
  - 用户首次提问（定义任务目标）
  - Tool 调用的输入/输出
  - 用户明确纠正或确认的信息（如 “不是 A，是 B”）
#### 示例结构
```python
history = [
{"role": "anchor", "content": "用户想订北京到上海的机票"},  # 锚点
{"role": "user", "content": "昨天说的航班有变动吗？"},
{"role": "assistant", "content": "CA1832 仍正常..."}
]
```
#### 适用场景
- 资源受限
- 通用聊天机器人

### 向量化记忆 + 动态检索（RAG for Chat History）

   将每轮对话嵌入向量库，每次生成前根据当前 query 检索最相关的过往片段。

#### 流程
- 每轮结束后，存入向量数据库
- 下次生成前：
  - 用当前用户消息做 embedding
  - 检索 Top-K 相关历史轮次
  - 拼接为：[检索结果] + [最近1–2轮]
#### 优势
  - 精准回忆早期细节
  - 支持话题跳跃


### 结构化状态管理（State Tracking）

   维护一个结构化对话状态对象，不传递原始对话。

#### 示例（订票场景）
```json

{
"task": "book_flight",
"slots": {
"origin": "Beijing",
"destination": "Shanghai",
"date": "2025-05-01",
"confirmed": false
},
"summary": "用户正在查询北京到上海5月1日的航班"
}
```
## 🛠️ 工程实践建议

1. **动态 Token 计算**
   使用模型对应的 tokenizer（如 Qwen 用 `tiktoken` + custom rules）实时统计 token。
2. **混合策略组合**
  * 日常对话：滑动窗口 + 锚点
  * 复杂任务：摘要压缩 + 结构化状态
  * 专业咨询：向量化记忆
3. **前端配合**
   UI 层也做历史折叠（如 “… earlier conversation summarized”），提升体验一致性。
4. **监控告警**
   记录因上下文截断导致的用户追问（如 “我之前说过…”），用于策略迭代。